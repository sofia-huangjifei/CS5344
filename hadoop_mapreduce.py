# -*- coding: utf-8 -*-
"""Hadoop MapReduce.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Iqzx9IduT_kfDakVjdUycTffBYfObcN7
"""

import csv
import os
import json
import argparse
import random
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

"""# Data Loading"""

scamdigger_dataset_path = "/content/drive/MyDrive/NUS MSBA/CS5344/script/scamdigger/scam_profiles_output.csv" #2021 onwards
datingnmore_dataset1_path = "/content/drive/MyDrive/NUS MSBA/CS5344/script/datingnmore/result_1_501.csv"
datingnmore_dataset2_path = "/content/drive/MyDrive/NUS MSBA/CS5344/script/datingnmore/result_500_1000.csv" #2018 onwards

scam_df = pd.read_csv(scamdigger_dataset_path)
dating_df1 = pd.read_csv(datingnmore_dataset1_path)
datingn_df2 = pd.read_csv(datingnmore_dataset1_path)

scam_df.shape, dating_df1.shape, datingn_df2.shape
# print(scam_df.dtypes)

datingnmore_df = pd.concat([dating_df1, datingn_df2], ignore_index=True)
datingnmore_df.shape

scam_df.head()

datingnmore_df.head()

scam_df.columns = scam_df.columns.str.lower()
scam_df = scam_df.rename(columns = {'looking':'target','purpose':'intent'})

scam_df['age'] = scam_df['age'].fillna('').astype(str).str.extract(r'(\d+)').astype(float)
datingnmore_df['age'] = datingnmore_df['age'].str.extract(r'(\d+)').astype(float)
scam_df['status'] = scam_df['status'].str.lower()
datingnmore_df = datingnmore_df[datingnmore_df['ethnicity'] != 'error']
scam_df['ethnicity'] = scam_df['ethnicity'].str.lower() #fill in nan value with mod later
scam_df['occupation'] = scam_df['occupation'].str.lower()
datingnmore_df['occupation'] = datingnmore_df['occupation'].str.lower() #fill '-' with mod later
datingnmore_df['location'] = datingnmore_df['location'].str.lower()
scam_df['location'] = scam_df['location'].str.lower()

scam_df.loc[scam_df['children'] == 'I have 1-2 children living with me','children'] = '1-2 living with me'
scam_df.loc[scam_df['children'] == 'I have 1-2 children living elsewhere','children'] = '1-2 living elsewhere'
scam_df.loc[scam_df['children'] == 'I don’t have children','children'] = 'no children'
scam_df.loc[scam_df['children'] == 'I want children','children'] = 'want children'
scam_df.loc[scam_df['children'] == 'I have 3 or or more children living elsewhere','children'] = 'more than 2 living elsewhere'
scam_df.loc[scam_df['children'] == 'I have 3 or more children living with me','children'] = 'more than 2 living with me'
scam_df.loc[scam_df['children'] == 'I don’t want children','children'] = "don't want children"
scam_df.loc[scam_df['children'] == 'don’t want children','children'] = "don't want children" #nan

def update_looking(value):
    if pd.isna(value):
        return value
    if 'Both' in value or 'Male, Female' in value:
        return 'Both'
    return value
scam_df['target'] = scam_df['target'].apply(update_looking)
datingnmore_df['target'] = datingnmore_df['target'].apply(update_looking)

def check_purposes(df):
    purposes = ['Friendship', 'Romance', 'Serious Relationship', 'Marriage', 'Fun']
    for purpose in purposes:
        # Create a new column for each purpose, assigning 1 if it's present, otherwise 0
        df[f'intent_{purpose.replace(" ", "_")}'] = df['intent'].apply(
            lambda x: 1 if isinstance(x, str) and purpose in x else 0
        )

    return df

scam_df = check_purposes(scam_df)
datingnmore_df = check_purposes(datingnmore_df)

scam_df = scam_df.drop('intent', axis=1)
datingnmore_df = datingnmore_df.drop('intent', axis=1)

scam_df = scam_df.dropna(subset=['age'])

scam_df_model = scam_df[['age','target','location','status','children','orientation',
                         'ethnicity','religion','occupation','description','intent_Friendship',
                         'intent_Romance','intent_Serious_Relationship','intent_Marriage','intent_Fun']]

datingnmore_df_model = datingnmore_df[['age','target','location','status','children','orientation',
                         'ethnicity','religion','occupation','description','intent_Friendship',
                         'intent_Romance','intent_Serious_Relationship','intent_Marriage','intent_Fun']]

scam_df_model['scammer']=1
datingnmore_df_model['scammer']=0

merge_df = pd.concat([scam_df_model, datingnmore_df_model], ignore_index=True)

merge_df.shape

def fill_missing_with_mode(df, columns):
    for col in columns:
        mode_value = df[col].mode()[0]
        df[col].fillna(mode_value, inplace=True)
    return df

columns_to_fill = ['status', 'target', 'children','orientation','ethnicity','religion']
merge_df = fill_missing_with_mode(merge_df, columns_to_fill)

merge_df['city'] = merge_df['location'].str.extract(r'([^,-,-–]+)')[0]
merge_df['city'] = merge_df['city'].str.replace(r'\d+', '', regex=True)
merge_df['city'] = merge_df['city'].str.strip()

"""# Description Analysis Using MapReduce"""

scammer_df_copy = scammer_df.copy()

desc = scammer_df_copy['description']
desc.head(10)

desc.to_csv('desc_input.txt', index=False, header=False)

import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
import spacy

# Download NLTK stopwords if not already downloaded
nltk.download('stopwords')

# NLTK stopwords
nltk_stopwords = set(stopwords.words('english'))

# Scikit-learn stopwords from CountVectorizer
vectorizer = CountVectorizer(stop_words='english')
sklearn_stopwords = set(vectorizer.get_stop_words())

# SpaCy stopwords
nlp = spacy.load("en_core_web_sm")
spacy_stopwords = set(nlp.Defaults.stop_words)

# Combine all stopwords into one set (automatically removes duplicates)
combined_stopwords = nltk_stopwords | sklearn_stopwords | spacy_stopwords

# Print the total number of combined stopwords
print(f"Total stopwords combined: {len(combined_stopwords)}")
print(combined_stopwords)

!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz
!tar -xzf hadoop-3.3.5.tar.gz
# install hadoop to /usr/local
!cp -r hadoop-3.3.5/ /usr/local/
import os
# hadoop needs to know where java is located
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
# the following line reduces the amount of Hadoop's printed output, to make it easier to see your own debug output
os.environ["HADOOP_ROOT_LOGGER"] = "WARN,console"

# Commented out IPython magic to ensure Python compatibility.
# #mapper
# %%file mapper.py
# import sys
# import re
# 
# 
# combined_stopwords = {'among', 'ourselves', 'was', 'mostly', "weren't", 'around', 'will', 'thereafter', 'due', 'by', 'could', 'themselves', 'be', 'o', 'least', 'ltd', 'perhaps', 'how', 'besides', '’d', 'seemed', 'have', 'therefore', "shouldn't", 'name', 'further', 'eg', 'ma', "you'd", 'become', 'un', 'nevertheless', 'thick', 'where', "don't",'dont','hadn', 'well', 'does', 'of', '’m', 've', 'doing', 'fill', 'ie', "'s", "that'll", 'mine', 'your', 'having', 'system', 'please', 'thereupon', 'keep', 'beforehand', 'same', 'seems', 'therein', 'amount', 'none', 'some', 'about', 'did', "should've", 'together', "you've", 'enough', 'or', 'forty', 'being', 'much', 'him', 'yet', 'whenever', 'each', 'would', 'can', 'hence', 'eleven', 'somehow', '’s', 'serious', 'so', 't', 'only', 'out', 'fire', 'seem', "couldn't", "'ve", 'who', 'an', 'such', 'anyone', '’ve', 'hereafter', 'five', 'you', 'etc', 'has', 'ever', 'we', 'its', '‘s', 'll', 'doesn', 'con', 'thereby', 'the', 'became', 'toward', 'hasn', "mightn't", 'rather', 'under', 'they', 'shan', 'while', 'beyond', 'using', 'do', 'latter', 'however', 'my', 'becomes', '‘d', 'anyhow', 'three', 'again', "hadn't", "you're", 'twelve', 'which', 'thru', 'hers', 'whose', "it's", '’ll', 'just', 'nine', 'full', 'anyway', 'yours', 'n’t', 'noone', 'side', 'used', "won't", 'he', 'm', 'hasnt', 'when', 'never', 'seeming', 'everywhere', 'de', 'if', 'go', 'via', '‘ll', 'than', 'back', 'other', 'interest', 'upon', 'former', 'cry', 'haven', 'what', 'get', 'latterly', 'mill', 'thin', 'find', 'alone', 'couldnt', "mustn't", 'with', "needn't", 'becoming', 'a', "shan't", 'off', "haven't", 'i', 'see', 'am', 'no', 'whereby', 'somewhere', 'at', "aren't", 'next', "'m", "wouldn't", 'either', 'but', 'didn', 'another', 'nowhere', 'others', 'is', 'because', 'very', 'formerly', 'and', 'before', 'hereupon', 'it', 'per', 'up', 'third', 'behind', 'their', 'aren', 'mustn', 'these', 'should', 'wasn', 'meanwhile', "'ll", 'four', '‘ve', 'had', 'nobody', 'once', 'now', 'front', "n't", 'nor', 'elsewhere', 'already', 'through', "doesn't", 'describe', 'sometimes', 'really', 'wherever', 'top', 'still', 'though', 'may', 'anywhere', 'ain', 'made', 'something', 'why', 'few', '’re', 'several', 'n‘t', 'found', 'beside', 'even', 'us', 'herself', "isn't", 'otherwise', 'wouldn', 'take', 'whole', 'although', 'throughout', 'over', 'last', 'theirs', 'on', '‘m', 'shouldn', 'always', 'cant', 'there', "hasn't", 'ca', 'unless', 'regarding', 'since', 'too', 'almost', 'wherein', 'mightn', 'fifteen', 'below', 'own', 'our', 'to', 'weren', 'first', "'d", 'must', 's', 'above', 'part', 'her', 'bill', 'sincere', 'd', 'more', 'whereas', 'herein', 'within', 'whereafter', 'nothing', 'twenty', 'across', 'anything', 'his', 'except', 'empty', 'are', 'as', 'else', 'call', 'yourself', 'onto', 'into', 'whom', 'might', 'most', 'amoungst', 'whoever', 'many', 'various', "she's", 'any', 'someone', 'himself', 'fifty', 'both', 'been', 'often', 'from', "wasn't", 'make', 'yourselves', 'hundred', 'done', 'co', 'whatever', 'me', 'ten', 'put', 'after', 'one', 'cannot', 'all', 'indeed', 'eight', 'myself', 'until', "'re", 'everyone', 'moreover', 'couldn', 'detail', 'afterwards', 'show', 'not', 'down', 'namely', 'move', 'needn', "didn't", 'here', 'then', 'amongst', 'everything', 'less', '‘re', 'quite', 'them', 'hereby', "you'll", 'along', 'between', 'every', 'that', 'towards', 'without', 'sometime', 'for', 'bottom', 'whence', 'were', 'she', 'sixty', 'against', 'this', 'in', 'won', 'thus', 'neither', 'six', 're', 'itself', 'also', 'thence', 'during', 'ours', 'give', 'say', 'inc', 'don', 'y', 'whether', 'whereupon', 'those', 'whither', 'isn', 'two','unknown'}
# 
# 
# for line in sys.stdin:
#     line = re.sub(r'[^\w\s]', '', line.lower())
# 
#     # Split line into words
#     words = line.split()
# 
#     # Emit word and count (only if not a stopword)
#     for word in words:
#         if word not in combined_stopwords and word.isalpha():  # check if word is alphabetic
#             print(f"{word}\t1")

# Commented out IPython magic to ensure Python compatibility.
# %%file reducer.py
# import sys
# 
# counts = {}
# 
# # Read input line by line from standard input
# for line in sys.stdin:
#     word, count = line.strip().split('\t')
#     count = int(count)
# 
#     # Update word count
#     if word in counts:
#         counts[word] += count
#     else:
#         counts[word] = count
# 
# # Sort the words by count in descending order
# sorted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)
# 
# # Output the top 30 most frequent words
# for word, count in sorted_counts[:30]:
#     print(f"{word}\t{count}")

!chmod u+rwx /content/mapper.py
!chmod u+rwx /content/reducer.py

!rm -rf /content/wordcount_output
!/usr/local/hadoop-3.3.5/bin/hadoop jar /usr/local/hadoop-3.3.5/share/hadoop/tools/lib/hadoop-streaming-3.3.5.jar \
-input desc_input.txt \
-output wordcount_output \
-file /content/mapper.py \
-file /content/reducer.py \
-mapper 'python mapper.py' \
-reducer 'python reducer.py'

!cat wordcount_output/part-00000



